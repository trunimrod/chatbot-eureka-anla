{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Constructor del Grafo de Conocimiento y Base Vectorial\n",
    "\n",
    "**Objetivo:** Procesar `eureka-merge.txt` para extraer documentos, entidades (leyes, palabras clave) y relaciones. Guarda una base de datos vectorial (ChromaDB) para la búsqueda inicial y un archivo de grafo (NetworkX) para explorar las conexiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 1: Instalación de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se necesitan librerías adicionales para el manejo de grafos\n",
    "!pip install -qU chromadb langchain langchain-community langchain-ollama langchain-text-splitters sentence-transformers networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 2: Importaciones y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import networkx as nx\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Parámetros de Configuración ---\n",
    "KNOWLEDGE_FILE = \"eureka-merge.txt\"\n",
    "CHROMA_PATH = \"./chroma_graph_db\"\n",
    "GRAPH_PATH = \"./knowledge_graph.gml\"\n",
    "EMBEDDING_MODEL = \"mxbai-embed-large\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 3: Función de Parseo del Archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_consolidated_file(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Parsea el archivo consolidado para extraer la estructura de cada documento.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.error(f\"El archivo de conocimiento '{file_path}' no fue encontrado.\")\n",
    "        return []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    docs_raw = re.split(r'# ARCHIVO \\\\d{2,}:', content)\n",
    "    parsed_documents = []\n",
    "\n",
    "    for i, doc_text in enumerate(docs_raw):\n",
    "        if len(doc_text.strip()) == 0 or 'FIN DE:' not in doc_text:\n",
    "            continue\n",
    "            \n",
    "        doc_id = f\"doc_{i}\"\n",
    "        \n",
    "        def extract_field(pattern, text):\n",
    "            # Patrón actualizado para manejar URL con < > o sin ellos\n",
    "            match = re.search(pattern, text, re.DOTALL)\n",
    "            return match.group(1).strip() if match else \"N/A\"\n",
    "            \n",
    "        def extract_list(pattern, text):\n",
    "            field = extract_field(pattern, text)\n",
    "            return [item.strip() for item in field.split('–') if item.strip()] if field != \"N/A\" else []\n",
    "\n",
    "        title = extract_field(r'# \\\\*\\\\*Título:\\\\*\\\\* (.+?)\\\\n', doc_text)\n",
    "        # --- LÍNEA NUEVA: Extracción de la URL ---\n",
    "        url = extract_field(r'\\\\*\\\\*URL:\\\\*\\\\* <(.+?)>', doc_text) \n",
    "        category = extract_field(r'### \\\\*\\\\*Categoría:\\\\*\\\\* (.+?)\\\\n', doc_text)\n",
    "        summary = extract_field(r'## \\\\*\\\\*Resumen\\\\*\\\\*\\\\n(.+?)### \\\\*\\\\*Fuente:\\\\*\\\\*', doc_text)\n",
    "        source = extract_field(r'### \\\\*\\\\*Fuente:\\\\*\\\\* (.+?)\\\\n', doc_text)\n",
    "        keywords = extract_list(r'## \\\\*\\\\*Palabras Claves\\\\*\\\\*\\\\n(.+?)## \\\\*\\\\*Concordancias\\\\*\\\\*', doc_text)\n",
    "        concordances = extract_list(r'## \\\\*\\\\*Concordancias\\\\*\\\\*\\\\n(.+?)############################################################', doc_text)\n",
    "        \n",
    "        parsed_documents.append({\n",
    "            'id': doc_id,\n",
    "            'title': title,\n",
    "            'url': url, # <-- Atributo URL añadido\n",
    "            'category': category,\n",
    "            'summary': summary,\n",
    "            'source': source,\n",
    "            'keywords': keywords,\n",
    "            'concordances': concordances,\n",
    "            'full_text': doc_text.strip()\n",
    "        })\n",
    "        \n",
    "    logger.info(f\"Parseados {len(parsed_documents)} documentos del archivo.\")\n",
    "    return parsed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 4: Construcción del Grafo y la Base de Datos Vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 16:51:54,871 - INFO - Parseados 284 documentos del archivo.\n",
      "2025-09-10 16:51:54,940 - INFO - ✅ Grafo de conocimiento guardado en './knowledge_graph.gml'. Nodos: 4134, Relaciones: 6480.\n",
      "2025-09-10 16:51:55,178 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "def build_knowledge_base():\n",
    "    \"\"\"Orquesta la creación del grafo y la base de datos vectorial.\"\"\"\n",
    "    documents_data = parse_consolidated_file(KNOWLEDGE_FILE)\n",
    "    if not documents_data:\n",
    "        return\n",
    "\n",
    "    # --- CAMBIO EN LA CREACIÓN DEL NODO ---\n",
    "    G = nx.Graph()\n",
    "    for doc in documents_data:\n",
    "        doc_id = doc['id']\n",
    "        # Añadimos la URL como un atributo del nodo\n",
    "        G.add_node(doc_id, \n",
    "                   type='Documento', \n",
    "                   title=doc['title'], \n",
    "                   summary=doc['summary'], \n",
    "                   url=doc['url']) # <-- Atributo URL añadido al nodo\n",
    "        \n",
    "        # El resto del código para crear relaciones no cambia...\n",
    "        for conc in doc['concordances']:\n",
    "            G.add_node(conc, type='Normativa')\n",
    "            G.add_edge(doc_id, conc, type='CONCORDANCIA')\n",
    "            \n",
    "        for keyword in doc['keywords']:\n",
    "            G.add_node(keyword, type='Keyword')\n",
    "            G.add_edge(doc_id, keyword, type='TIENE_KEYWORD')\n",
    "            \n",
    "    nx.write_gml(G, GRAPH_PATH)\n",
    "    logger.info(f\"✅ Grafo de conocimiento guardado en '{GRAPH_PATH}'.\")\n",
    "\n",
    "    # El resto del código para crear la base vectorial no cambia...\n",
    "    docs_for_vectorstore = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    \n",
    "    for doc in documents_data:\n",
    "        chunks = text_splitter.split_text(doc['full_text'])\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            metadata = {'doc_id': doc['id'], 'title': doc['title'], 'chunk_num': i}\n",
    "            docs_for_vectorstore.append(Document(page_content=chunk, metadata=metadata))\n",
    "    \n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs_for_vectorstore,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    logger.info(f\"✅ Base de datos vectorial guardada en '{CHROMA_PATH}'.\")\n",
    "\n",
    "# Ejecutar el proceso de construcción\n",
    "build_knowledge_base()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
