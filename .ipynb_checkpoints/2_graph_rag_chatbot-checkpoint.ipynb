{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Chatbot Eureka con Graph RAG\n",
    "\n",
    "**Objetivo:** Cargar el grafo y la base de datos vectorial para ejecutar un chatbot que aprovecha las relaciones entre documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 1: Crear Archivo de la Aplicaci√≥n Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_chatbot.py\n",
    "import streamlit as st\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Optional, Dict, List\n",
    "from dataclasses import dataclass\n",
    "import networkx as nx\n",
    "\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from prompts import EUREKA_PROMPT_TEMPLATE\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class EurekaConfig:\n",
    "    embedding_model: str = \"mxbai-embed-large\"\n",
    "    llm_model: str = \"llama3.2\"\n",
    "    chroma_path: str = \"./chroma_graph_db\"\n",
    "    graph_path: str = \"./knowledge_graph.gml\"\n",
    "    temperature: float = 0.1\n",
    "\n",
    "class EurekaGraphChatbot:\n",
    "    def __init__(self, config: EurekaConfig):\n",
    "        self.config = config\n",
    "        self.vectorstore = None\n",
    "        self.graph = None\n",
    "        self.llm = ChatOllama(model=config.llm_model, temperature=config.temperature)\n",
    "        self.prompt = PromptTemplate(template=EUREKA_PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    def initialize(self):\n",
    "        if not (Path(self.config.chroma_path).exists() and Path(self.config.graph_path).exists()):\n",
    "            raise FileNotFoundError(\"Base de datos no encontrada. Ejecuta '1_build_graph_database.ipynb' primero.\")\n",
    "        \n",
    "        embeddings = OllamaEmbeddings(model=self.config.embedding_model)\n",
    "        self.vectorstore = Chroma(persist_directory=self.config.chroma_path, embedding_function=embeddings)\n",
    "        self.graph = nx.read_gml(self.config.graph_path)\n",
    "        logger.info(\"‚úÖ Base de datos vectorial y grafo de conocimiento cargados.\")\n",
    "\n",
    "    def get_graph_context(self, doc_id: str) -> str:\n",
    "        if doc_id not in self.graph:\n",
    "            return \"\"\n",
    "        \n",
    "        node_data = self.graph.nodes[doc_id]\n",
    "        context_parts = [f\"**Documento Principal:** {node_data.get('title', 'N/A')}\\n**Resumen:** {node_data.get('summary', 'N/A')}\"]\n",
    "        \n",
    "        neighbors = list(self.graph.neighbors(doc_id))\n",
    "        if neighbors:\n",
    "            concordancias = [n for n in neighbors if self.graph.nodes[n].get('type') == 'Normativa']\n",
    "            if concordancias:\n",
    "                context_parts.append(f\"\\n**Normativas Relacionadas (Concordancias):**\\n- \" + \"\\n- \".join(concordancias[:5]))\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "\n",
    "    def query(self, question: str) -> Dict:\n",
    "        if not self.vectorstore or not self.graph:\n",
    "            raise ValueError(\"Sistema no inicializado.\")\n",
    "\n",
    "        # 1. B√∫squeda vectorial para encontrar el punto de entrada al grafo\n",
    "        results = self.vectorstore.similarity_search(question, k=1)\n",
    "        if not results:\n",
    "            return {\"answer\": \"No encontr√© informaci√≥n relevante.\", \"input_tokens\": 0, \"output_tokens\": 0}\n",
    "\n",
    "        entry_doc_id = results[0].metadata['doc_id']\n",
    "\n",
    "        # 2. Usar el grafo para enriquecer el contexto\n",
    "        graph_context = self.get_graph_context(entry_doc_id)\n",
    "\n",
    "        # 3. Generar respuesta con el LLM\n",
    "        prompt_formatted = self.prompt.format(context=graph_context, question=question)\n",
    "        response_obj = self.llm.invoke(prompt_formatted)\n",
    "        answer = response_obj.content\n",
    "        metadata = response_obj.response_metadata\n",
    "\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"input_tokens\": metadata.get('prompt_eval_count', 0),\n",
    "            \"output_tokens\": metadata.get('eval_count', 0)\n",
    "        }\n",
    "\n",
    "# --- Aplicaci√≥n Streamlit ---\n",
    "\n",
    "st.set_page_config(page_title=\"Eureka Graph RAG\", page_icon=\"üåø\", layout=\"wide\")\n",
    "\n",
    "@st.cache_resource\n",
    "def initialize_chatbot():\n",
    "    try:\n",
    "        config = EurekaConfig()\n",
    "        chatbot = EurekaGraphChatbot(config)\n",
    "        chatbot.initialize()\n",
    "        return chatbot\n",
    "    except FileNotFoundError as e:\n",
    "        st.error(f\"üö® Error Cr√≠tico: {e}\")\n",
    "        return None\n",
    "\n",
    "eureka_chatbot = initialize_chatbot()\n",
    "\n",
    "# ... [El resto de la UI de Streamlit es id√©ntica a la versi√≥n anterior] ...\n",
    "# ... Header, Sidebar, M√©tricas, Chat loop ...\n",
    "col1, col2 = st.columns([1, 4])\n",
    "with col1: st.image(\"https://www.anla.gov.co/images/logo-anla.png\", width=120)\n",
    "with col2: \n",
    "    st.title(\"üåø Eureka: Asistente con Grafo de Conocimiento\")\n",
    "    st.markdown(\"**Ahora con entendimiento de relaciones entre documentos**\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"‚ÑπÔ∏è Sobre Eureka (Graph RAG)\")\n",
    "    # ... (resto de la UI sin cambios)\n",
    "    st.info(\"Soy tu asistente para facilitar el acceso a informaci√≥n sobre licenciamiento y participaci√≥n ciudadana. ¬°Preg√∫ntame!\")\n",
    "    st.header(\"üìä Estad√≠sticas de Sesi√≥n\")\n",
    "    if 'log' in st.session_state and st.session_state.log:\n",
    "        # C√≥digo de m√©tricas...\n",
    "    else:\n",
    "        st.write(\"A√∫n no hay interacciones.\")\n",
    "\n",
    "if not eureka_chatbot:\n",
    "    st.stop()\n",
    "\n",
    "if \"messages\" not in st.session_state: st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"¬°Hola! Soy Eureka, ahora con un grafo de conocimiento para darte respuestas m√°s completas. ¬øEn qu√© te puedo ayudar?\"}]\n",
    "for message in st.session_state.messages: \n",
    "    with st.chat_message(message[\"role\"]): st.markdown(message[\"content\"])\n",
    "\n",
    "if prompt := st.chat_input(\"Escribe tu consulta aqu√≠...\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"): st.markdown(prompt)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Consultando el grafo de conocimiento...\"):\n",
    "            start_time = time.time()\n",
    "            result = eureka_chatbot.query(prompt)\n",
    "            response_time = time.time() - start_time\n",
    "            # log_interaction(prompt, result, response_time)\n",
    "            response = result[\"answer\"]\n",
    "            st.markdown(response)\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    st.rerun()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 2: Instrucciones de Ejecuci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"Para iniciar, ejecuta en tu terminal: streamlit run app_chatbot.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}