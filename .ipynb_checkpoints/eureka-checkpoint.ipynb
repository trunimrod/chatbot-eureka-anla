{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Optimizado: Backend RAG para el Chatbot \"Eureka\" de la ANLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 1: Configuración e Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Instalar dependencias optimizadas\n",
    "!pip install -qU chromadb==0.4.15 langchain==0.0.350 langchain_community langchain_ollama langchain_text_splitters streamlit python-dotenv sentence-transformers\n",
    "\n",
    "# Verificar que Ollama está disponible\n",
    "def check_ollama():\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            logger.info(\"✅ Ollama está disponible\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(\"❌ Ollama no está disponible o no responde\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"❌ Ollama no está instalado\")\n",
    "        return False\n",
    "\n",
    "check_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 2: Base de Conocimiento Expandida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile eureka-knowledge.txt\n",
    "# Base de Conocimiento Completa para Eureka - ANLA\n",
    "\n",
    "## Información Institucional\n",
    "La Autoridad Nacional de Licencias Ambientales (ANLA) es la entidad encargada de que los proyectos, obras o actividades sujetos a licenciamiento, permiso o trámite ambiental en Colombia cumplan con la normativa ambiental nacional.\n",
    "\n",
    "### Misión\n",
    "Contribuir al desarrollo sostenible del país mediante la evaluación, seguimiento y control de los proyectos, obras o actividades sujetos a licenciamiento, permiso o trámite ambiental.\n",
    "\n",
    "### Visión\n",
    "Ser reconocida como una entidad líder en la evaluación y control de proyectos con impacto ambiental.\n",
    "\n",
    "## Licenciamiento Ambiental\n",
    "\n",
    "### ¿Qué es?\n",
    "El licenciamiento ambiental es la autorización que otorga la autoridad ambiental competente para la ejecución de un proyecto, obra o actividad que pueda producir deterioro grave a los recursos naturales renovables o al medio ambiente.\n",
    "\n",
    "### Tipos de instrumentos:\n",
    "1. **Licencia Ambiental**: Para proyectos de gran impacto\n",
    "2. **Permisos Ambientales**: Para actividades específicas\n",
    "3. **Concesiones**: Para uso de recursos naturales\n",
    "4. **Autorizaciones**: Para manejo de fauna y flora\n",
    "\n",
    "### Sectores que requieren licencia:\n",
    "- Hidrocarburos\n",
    "- Minería\n",
    "- Infraestructura vial\n",
    "- Energía eléctrica\n",
    "- Portuario\n",
    "- Agroindustrial\n",
    "\n",
    "## Participación Ciudadana\n",
    "\n",
    "### Marco Legal\n",
    "La participación ciudadana está fundamentada en:\n",
    "- Artículo 79 de la Constitución Nacional\n",
    "- Ley 99 de 1993\n",
    "- Decreto 1076 de 2015\n",
    "- Resolución 1552 de 2005\n",
    "\n",
    "### Mecanismos Principales\n",
    "\n",
    "#### 1. Audiencias Públicas Ambientales\n",
    "**¿Qué son?**\n",
    "Espacios de diálogo entre la autoridad ambiental, el solicitante de la licencia y la comunidad.\n",
    "\n",
    "**¿Quién puede solicitarlas?**\n",
    "- Procurador General de la Nación\n",
    "- Defensor del Pueblo  \n",
    "- Ministros\n",
    "- Gobernadores\n",
    "- Alcaldes\n",
    "- Al menos 100 personas\n",
    "- 3 organizaciones cívicas\n",
    "\n",
    "**Información completa**: https://www.anla.gov.co/participacion\n",
    "\n",
    "#### 2. Terceros Intervinientes\n",
    "**¿Qué es?**\n",
    "Mecanismo que permite a personas naturales o jurídicas participar en procedimientos administrativos cuando tienen un interés jurídico directo.\n",
    "\n",
    "**Requisitos**:\n",
    "- Demostrar interés directo en la decisión\n",
    "- Radicar solicitud dentro de los términos establecidos\n",
    "- Aportar documentos que soporten el interés\n",
    "\n",
    "#### 3. Derecho de Petición\n",
    "Cualquier persona puede presentar peticiones respetuosas para:\n",
    "- Obtener información\n",
    "- Solicitar actuaciones\n",
    "- Formular consultas\n",
    "- Presentar quejas o reclamos\n",
    "\n",
    "**Canal oficial**: VITAL (Ventanilla Integral de Trámites Ambientales en Línea)\n",
    "\n",
    "#### 4. Consulta de Expedientes\n",
    "Los ciudadanos pueden consultar:\n",
    "- Estado de trámites\n",
    "- Documentos técnicos\n",
    "- Conceptos emitidos\n",
    "- Resoluciones\n",
    "\n",
    "**Acceso**: https://www.anla.gov.co/expedientes\n",
    "\n",
    "## Información de Contacto\n",
    "- **Sitio web**: https://www.anla.gov.co\n",
    "- **VITAL**: https://vital.anla.gov.co\n",
    "- **Línea de atención**: (601) 254 8888\n",
    "- **Correo institucional**: info@anla.gov.co\n",
    "- **Dirección**: Calle 37 # 8 - 40, Bogotá D.C.\n",
    "\n",
    "## Horarios de Atención\n",
    "- **Presencial**: Lunes a viernes de 8:00 AM a 5:00 PM\n",
    "- **Virtual (VITAL)**: 24 horas, 7 días a la semana\n",
    "- **Línea telefónica**: Lunes a viernes de 8:00 AM a 5:00 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 3: Clase RAG Optimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from typing import List, Optional, Dict\n",
    "from dataclasses import dataclass\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain.schema import Document\n",
    "\n",
    "@dataclass\n",
    "class EurekaConfig:\n",
    "    \"\"\"Configuración para el sistema Eureka\"\"\"\n",
    "    chunk_size: int = 800\n",
    "    chunk_overlap: int = 100\n",
    "    embedding_model: str = \"mxbai-embed-large\"\n",
    "    llm_model: str = \"llama3.2\"\n",
    "    persist_directory: str = \"./eureka_optimized_db\"\n",
    "    retriever_k: int = 5\n",
    "    temperature: float = 0.1\n",
    "\n",
    "class EurekaRAG:\n",
    "    def __init__(self, config: EurekaConfig):\n",
    "        self.config = config\n",
    "        self.vectorstore: Optional[Chroma] = None\n",
    "        self.qa_chain: Optional[RetrievalQA] = None\n",
    "        self.embeddings = OllamaEmbeddings(model=config.embedding_model)\n",
    "        self.llm = ChatOllama(\n",
    "            model=config.llm_model,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        \n",
    "        # Prompt optimizado con mejor estructura\n",
    "        self.prompt_template = \"\"\"Eres \"Eureka\", el asistente virtual oficial de la ANLA (Autoridad Nacional de Licencias Ambientales).\n",
    "\n",
    "PERSONALIDAD Y TONO:\n",
    "- Servicial, claro, profesional y confiable\n",
    "- Usa un lenguaje accesible para ciudadanos\n",
    "- Siempre mantén un tono institucional respetuoso\n",
    "\n",
    "REGLAS ESTRICTAS:\n",
    "1. SOLO responde basándote en el CONTEXTO proporcionado\n",
    "2. Si la información no está en el contexto, usa EXACTAMENTE esta respuesta:\n",
    "   \"Gracias por tu consulta. No tengo información específica sobre ese tema en mi base de conocimiento actual. \n",
    "   Te invito a contactar los canales oficiales de la ANLA:\n",
    "   - VITAL: https://vital.anla.gov.co\n",
    "   - Línea: (601) 254 8888\n",
    "   - Sitio web: https://www.anla.gov.co\"\n",
    "\n",
    "3. Si el contexto contiene enlaces, SIEMPRE inclúyelos al final\n",
    "4. Para consultas sobre procedimientos, siempre menciona VITAL\n",
    "5. No proporciones asesoría legal, solo información institucional\n",
    "\n",
    "CONTEXTO RELEVANTE:\n",
    "{context}\n",
    "\n",
    "PREGUNTA DEL USUARIO:\n",
    "{question}\n",
    "\n",
    "RESPUESTA DE EUREKA:\"\"\"\n",
    "\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=self.prompt_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "    def load_documents(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Carga y procesa documentos con manejo de errores mejorado\"\"\"\n",
    "        try:\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Mejorar metadatos\n",
    "            for doc in documents:\n",
    "                doc.metadata.update({\n",
    "                    'source': file_path,\n",
    "                    'content_hash': hashlib.md5(doc.page_content.encode()).hexdigest()\n",
    "                })\n",
    "            \n",
    "            # Divisor optimizado\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.config.chunk_size,\n",
    "                chunk_overlap=self.config.chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            split_docs = splitter.split_documents(documents)\n",
    "            logger.info(f\"✅ Procesados {len(split_docs)} chunks desde {file_path}\")\n",
    "            return split_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error cargando documentos: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_vectorstore(self, documents: List[Document]) -> None:\n",
    "        \"\"\"Crea la base de datos vectorial con optimizaciones\"\"\"\n",
    "        try:\n",
    "            # Verificar si ya existe\n",
    "            if Path(self.config.persist_directory).exists():\n",
    "                logger.info(\"📂 Cargando base de datos existente...\")\n",
    "                self.vectorstore = Chroma(\n",
    "                    persist_directory=self.config.persist_directory,\n",
    "                    embedding_function=self.embeddings\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\"🏗️ Creando nueva base de datos vectorial...\")\n",
    "                self.vectorstore = Chroma.from_documents(\n",
    "                    documents=documents,\n",
    "                    embedding=self.embeddings,\n",
    "                    persist_directory=self.config.persist_directory,\n",
    "                    collection_metadata={\"description\": \"Eureka ANLA Knowledge Base\"}\n",
    "                )\n",
    "                \n",
    "            logger.info(f\"✅ Base vectorial lista con {self.vectorstore._collection.count()} documentos\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error creando vectorstore: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def setup_qa_chain(self) -> None:\n",
    "        \"\"\"Configura la cadena de QA optimizada\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Vectorstore no inicializado\")\n",
    "            \n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",  # Máxima relevancia marginal\n",
    "            search_kwargs={\n",
    "                \"k\": self.config.retriever_k,\n",
    "                \"fetch_k\": self.config.retriever_k * 2,\n",
    "                \"lambda_mult\": 0.7\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,  # Para debugging\n",
    "            chain_type_kwargs={\"prompt\": self.prompt},\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        logger.info(\"✅ Cadena QA configurada\")\n",
    "\n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Procesa consulta con manejo de casos especiales\"\"\"\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"QA chain no inicializada\")\n",
    "            \n",
    "        # Modo administrador\n",
    "        if question.strip().lower().startswith(\"admin:\"):\n",
    "            return {\n",
    "                \"answer\": \"\"\"🔧 **Información Técnica de Eureka**\n",
    "\n",
    "**Arquitectura**: RAG (Retrieval-Augmented Generation)\n",
    "**Embeddings**: mxbai-embed-large via Ollama\n",
    "**LLM**: Llama 3.2 via Ollama  \n",
    "**Base de datos**: ChromaDB vectorial\n",
    "**Búsqueda**: MMR (Máxima Relevancia Marginal)\n",
    "**Framework**: LangChain + Streamlit\n",
    "\n",
    "**Flujo de procesamiento**:\n",
    "1. Análisis de intención de la consulta\n",
    "2. Búsqueda semántica en knowledge base\n",
    "3. Selección de contexto relevante\n",
    "4. Generación de respuesta estructurada\n",
    "5. Aplicación de filtros de calidad\"\"\",\n",
    "                \"sources\": [],\n",
    "                \"type\": \"admin\"\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            result = self.qa_chain.invoke({\"query\": question.strip()})\n",
    "            return {\n",
    "                \"answer\": result[\"result\"],\n",
    "                \"sources\": [doc.metadata.get(\"source\", \"\") for doc in result[\"source_documents\"]],\n",
    "                \"type\": \"standard\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error procesando consulta: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Disculpa, ocurrió un error procesando tu consulta. Por favor intenta de nuevo o contacta soporte técnico.\",\n",
    "                \"sources\": [],\n",
    "                \"type\": \"error\"\n",
    "            }\n",
    "\n",
    "    def build_system(self, knowledge_file: str) -> None:\n",
    "        \"\"\"Construye el sistema completo\"\"\"\n",
    "        logger.info(\"🚀 Iniciando construcción del sistema Eureka...\")\n",
    "        \n",
    "        # 1. Cargar documentos\n",
    "        documents = self.load_documents(knowledge_file)\n",
    "        \n",
    "        # 2. Crear vectorstore\n",
    "        self.create_vectorstore(documents)\n",
    "        \n",
    "        # 3. Configurar QA\n",
    "        self.setup_qa_chain()\n",
    "        \n",
    "        logger.info(\"✅ Sistema Eureka listo para consultas\")\n",
    "\n",
    "# Inicializar sistema\n",
    "config = EurekaConfig()\n",
    "eureka = EurekaRAG(config)\n",
    "eureka.build_system(\"eureka-knowledge.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 4: Testing y Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suite de pruebas completa\n",
    "def test_eureka_system():\n",
    "    \"\"\"Pruebas automatizadas del sistema\"\"\"\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"query\": \"¿Qué es una licencia ambiental?\",\n",
    "            \"expected_keywords\": [\"licencia\", \"ambiental\", \"autorización\"],\n",
    "            \"should_contain_links\": False\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"¿Cómo puedo participar en una audiencia pública?\",\n",
    "            \"expected_keywords\": [\"audiencia\", \"pública\", \"participación\"],\n",
    "            \"should_contain_links\": True\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"¿Cuál es el precio del Bitcoin hoy?\",\n",
    "            \"expected_keywords\": [\"no tengo información\", \"VITAL\"],\n",
    "            \"should_contain_links\": True\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"admin: ¿cómo funcionas?\",\n",
    "            \"expected_keywords\": [\"RAG\", \"ChromaDB\", \"Llama\"],\n",
    "            \"should_contain_links\": False\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🧪 Iniciando pruebas del sistema Eureka...\\n\" + \"=\"*60)\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\n**Prueba {i}**: {test['query']}\")\n",
    "        result = eureka.query(test['query'])\n",
    "        answer = result['answer']\n",
    "        \n",
    "        print(f\"**Respuesta**: {answer[:200]}...\")\n",
    "        \n",
    "        # Validar keywords esperadas\n",
    "        keywords_found = sum(1 for keyword in test['expected_keywords'] \n",
    "                             if keyword.lower() in answer.lower())\n",
    "        print(f\"**Keywords encontradas**: {keywords_found}/{len(test['expected_keywords'])}\")\n",
    "        \n",
    "        # Validar enlaces\n",
    "        has_links = \"http\" in answer\n",
    "        links_ok = has_links == test['should_contain_links']\n",
    "        print(f\"**Enlaces**: {'✅' if links_ok else '❌'}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Ejecutar pruebas\n",
    "test_eureka_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 5: Aplicación Streamlit Mejorada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_eureka_optimized.py\n",
    "import streamlit as st\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from typing import List, Optional, Dict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Dependencias de LangChain (copiadas desde el notebook)\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Configurar un logger básico para el script de Streamlit\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Copia de la Clase EurekaRAG y su Configuración ---\n",
    "@dataclass\n",
    "class EurekaConfig:\n",
    "    chunk_size: int = 800\n",
    "    chunk_overlap: int = 100\n",
    "    embedding_model: str = \"mxbai-embed-large\"\n",
    "    llm_model: str = \"llama3.2\"\n",
    "    persist_directory: str = \"./eureka_optimized_db\"\n",
    "    retriever_k: int = 5\n",
    "    temperature: float = 0.1\n",
    "\n",
    "class EurekaRAG:\n",
    "    def __init__(self, config: EurekaConfig):\n",
    "        self.config = config\n",
    "        self.vectorstore: Optional[Chroma] = None\n",
    "        self.qa_chain: Optional[RetrievalQA] = None\n",
    "        self.embeddings = OllamaEmbeddings(model=config.embedding_model)\n",
    "        self.llm = ChatOllama(\n",
    "            model=config.llm_model,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        self.prompt_template = \"\"\"Eres \"Eureka\", el asistente virtual oficial de la ANLA (Autoridad Nacional de Licencias Ambientales).\n",
    "\n",
    "PERSONALIDAD Y TONO:\n",
    "- Servicial, claro, profesional y confiable\n",
    "- Usa un lenguaje accesible para ciudadanos\n",
    "- Siempre mantén un tono institucional respetuoso\n",
    "\n",
    "REGLAS ESTRICTAS:\n",
    "1. SOLO responde basándote en el CONTEXTO proporcionado\n",
    "2. Si la información no está en el contexto, usa EXACTAMENTE esta respuesta:\n",
    "   \"Gracias por tu consulta. No tengo información específica sobre ese tema en mi base de conocimiento actual. \n",
    "   Te invito a contactar los canales oficiales de la ANLA:\n",
    "   - VITAL: https://vital.anla.gov.co\n",
    "   - Línea: (601) 254 8888\n",
    "   - Sitio web: https://www.anla.gov.co\"\n",
    "\n",
    "3. Si el contexto contiene enlaces, SIEMPRE inclúyelos al final\n",
    "4. Para consultas sobre procedimientos, siempre menciona VITAL\n",
    "5. No proporciones asesoría legal, solo información institucional\n",
    "\n",
    "CONTEXTO RELEVANTE:\n",
    "{context}\n",
    "\n",
    "PREGUNTA DEL USUARIO:\n",
    "{question}\n",
    "\n",
    "RESPUESTA DE EUREKA:\"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=self.prompt_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "    def load_documents(self, file_path: str) -> List[Document]:\n",
    "        try:\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            for doc in documents:\n",
    "                doc.metadata.update({\n",
    "                    'source': file_path,\n",
    "                    'content_hash': hashlib.md5(doc.page_content.encode()).hexdigest()\n",
    "                })\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.config.chunk_size,\n",
    "                chunk_overlap=self.config.chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "            )\n",
    "            split_docs = splitter.split_documents(documents)\n",
    "            logger.info(f\"✅ Procesados {len(split_docs)} chunks desde {file_path}\")\n",
    "            return split_docs\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error cargando documentos: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_vectorstore(self, documents: List[Document]) -> None:\n",
    "        try:\n",
    "            if Path(self.config.persist_directory).exists():\n",
    "                logger.info(\"📂 Cargando base de datos existente...\")\n",
    "                self.vectorstore = Chroma(\n",
    "                    persist_directory=self.config.persist_directory,\n",
    "                    embedding_function=self.embeddings\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\"🏗️ Creando nueva base de datos vectorial...\")\n",
    "                self.vectorstore = Chroma.from_documents(\n",
    "                    documents=documents,\n",
    "                    embedding=self.embeddings,\n",
    "                    persist_directory=self.config.persist_directory,\n",
    "                    collection_metadata={\"description\": \"Eureka ANLA Knowledge Base\"}\n",
    "                )\n",
    "            logger.info(f\"✅ Base vectorial lista con {self.vectorstore._collection.count()} documentos\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error creando vectorstore: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def setup_qa_chain(self) -> None:\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Vectorstore no inicializado\")\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": self.config.retriever_k,\n",
    "                \"fetch_k\": self.config.retriever_k * 2,\n",
    "                \"lambda_mult\": 0.7\n",
    "            }\n",
    "        )\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.prompt},\n",
    "            verbose=False\n",
    "        )\n",
    "        logger.info(\"✅ Cadena QA configurada\")\n",
    "\n",
    "    def query(self, question: str) -> Dict:\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"QA chain no inicializada\")\n",
    "        if question.strip().lower().startswith(\"admin:\"):\n",
    "            return {\n",
    "                \"answer\": \"\"\"🔧 **Información Técnica de Eureka**\n",
    "\n",
    "**Arquitectura**: RAG (Retrieval-Augmented Generation)\n",
    "**Embeddings**: mxbai-embed-large via Ollama\n",
    "**LLM**: Llama 3.2 via Ollama  \n",
    "**Base de datos**: ChromaDB vectorial\n",
    "**Búsqueda**: MMR (Máxima Relevancia Marginal)\n",
    "**Framework**: LangChain + Streamlit\n",
    "\n",
    "**Flujo de procesamiento**:\n",
    "1. Análisis de intención de la consulta\n",
    "2. Búsqueda semántica en knowledge base\n",
    "3. Selección de contexto relevante\n",
    "4. Generación de respuesta estructurada\n",
    "5. Aplicación de filtros de calidad\"\"\",\n",
    "                \"sources\": [],\n",
    "                \"type\": \"admin\"\n",
    "            }\n",
    "        try:\n",
    "            result = self.qa_chain.invoke({\"query\": question.strip()})\n",
    "            return {\n",
    "                \"answer\": result[\"result\"],\n",
    "                \"sources\": [doc.metadata.get(\"source\", \"\") for doc in result[\"source_documents\"]],\n",
    "                \"type\": \"standard\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error procesando consulta: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Disculpa, ocurrió un error procesando tu consulta. Por favor intenta de nuevo o contacta soporte técnico.\",\n",
    "                \"sources\": [],\n",
    "                \"type\": \"error\"\n",
    "            }\n",
    "\n",
    "    def build_system(self, knowledge_file: str) -> None:\n",
    "        logger.info(\"🚀 Iniciando construcción del sistema Eureka...\")\n",
    "        documents = self.load_documents(knowledge_file)\n",
    "        self.create_vectorstore(documents)\n",
    "        self.setup_qa_chain()\n",
    "        logger.info(\"✅ Sistema Eureka listo para consultas\")\n",
    "\n",
    "# --- Fin de la Copia ---\n",
    "\n",
    "# Configuración de página\n",
    "st.set_page_config(\n",
    "    page_title=\"Eureka - ANLA\",\n",
    "    page_icon=\"🌿\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# CSS personalizado\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        padding: 1rem 0;\n",
    "        border-bottom: 2px solid #2E8B57;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .chat-message {\n",
    "        padding: 1rem;\n",
    "        margin: 0.5rem 0;\n",
    "        border-radius: 0.5rem;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #f0f2f6;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #e8f5e8;\n",
    "    }\n",
    "    .metrics-container {\n",
    "        background-color: #f8f9fa;\n",
    "        padding: 1rem;\n",
    "        border-radius: 0.5rem;\n",
    "        margin: 1rem 0;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Inicialización del sistema (cached)\n",
    "@st.cache_resource\n",
    "def initialize_eureka():\n",
    "    config = EurekaConfig()\n",
    "    eureka = EurekaRAG(config)\n",
    "    # Asegurarse de que el archivo de conocimiento exista\n",
    "    knowledge_file = \"eureka-knowledge.txt\"\n",
    "    if not Path(knowledge_file).exists():\n",
    "        st.error(f\"El archivo de conocimiento '{knowledge_file}' no fue encontrado. Asegúrate de ejecutar la Celda 2 del notebook.\")\n",
    "        return None\n",
    "    eureka.build_system(knowledge_file)\n",
    "    return eureka\n",
    "\n",
    "# Función para logging de métricas\n",
    "def log_interaction(query: str, response: str, response_time: float):\n",
    "    \"\"\"Log de interacciones para análisis posterior\"\"\"\n",
    "    if \"interaction_log\" not in st.session_state:\n",
    "        st.session_state.interaction_log = []\n",
    "    \n",
    "    st.session_state.interaction_log.append({\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"query\": query,\n",
    "        \"response_length\": len(response),\n",
    "        \"response_time\": response_time,\n",
    "        \"session_id\": st.session_state.get(\"session_id\", \"unknown\")\n",
    "    })\n",
    "\n",
    "# Header principal\n",
    "st.markdown('<div class=\"main-header\">', unsafe_allow_html=True)\n",
    "col1, col2 = st.columns([1, 4])\n",
    "with col1:\n",
    "    st.image(\"https://www.anla.gov.co/images/logo-anla.png\", width=120)\n",
    "with col2:\n",
    "    st.title(\"🌿 Eureka\")\n",
    "    st.markdown(\"**Tu asistente virtual de la ANLA**\")\n",
    "st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "# Sidebar con información\n",
    "with st.sidebar:\n",
    "    st.header(\"ℹ️ Información\")\n",
    "    st.info(\"\"\"\n",
    "    **Eureka** es tu asistente virtual oficial de la ANLA.\n",
    "    \n",
    "    **Puedo ayudarte con:**\n",
    "    - Licenciamiento ambiental\n",
    "    - Participación ciudadana\n",
    "    - Información institucional\n",
    "    - Trámites y procedimientos\n",
    "    \"\"\" )\n",
    "    \n",
    "    st.header(\"📊 Estadísticas de Sesión\")\n",
    "    if \"interaction_log\" in st.session_state and st.session_state.interaction_log:\n",
    "        total_queries = len(st.session_state.interaction_log)\n",
    "        if total_queries > 0:\n",
    "            avg_response_time = sum(log[\"response_time\"] for log in st.session_state.interaction_log) / total_queries\n",
    "            st.metric(\"Consultas realizadas\", total_queries)\n",
    "            st.metric(\"Tiempo promedio respuesta\", f\"{avg_response_time:.2f}s\")\n",
    "    else:\n",
    "        st.metric(\"Consultas realizadas\", 0)\n",
    "        st.metric(\"Tiempo promedio respuesta\", \"0.00s\")\n",
    "\n",
    "# Inicializar sistema\n",
    "eureka = initialize_eureka()\n",
    "\n",
    "# Generar session ID único\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = str(hash(datetime.now().isoformat()))\n",
    "\n",
    "# Inicializar chat\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [{\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": \"\"\"¡Hola! 👋 Soy **Eureka**, tu asistente virtual de la ANLA.\n",
    "\n",
    "Estoy aquí para ayudarte con información sobre:\n",
    "- 📋 Licenciamiento ambiental\n",
    "- 🗣️ Participación ciudadana  \n",
    "- 🏛️ Información institucional\n",
    "- 📝 Trámites y procedimientos\n",
    "\n",
    "¿En qué puedo colaborarte hoy?\"\"\"\n",
    "    }]\n",
    "\n",
    "# Mostrar historial de chat\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Input del usuario\n",
    "if prompt := st.chat_input(\"💬 Escribe tu consulta aquí...\"):\n",
    "    # Agregar mensaje del usuario\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Generar respuesta\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        if eureka:\n",
    "            with st.spinner(\"🤔 Procesando tu consulta...\"):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    result = eureka.query(prompt)\n",
    "                    response = result[\"answer\"]\n",
    "                    response_time = time.time() - start_time\n",
    "                    \n",
    "                    # Log de interacción\n",
    "                    log_interaction(prompt, response, response_time)\n",
    "                    \n",
    "                    st.markdown(response)\n",
    "                    \n",
    "                    # Mostrar fuentes si están disponibles (modo debug)\n",
    "                    if st.sidebar.checkbox(\"Mostrar información de debug\") and result.get(\"sources\"):\n",
    "                        with st.expander(\"🔍 Fuentes consultadas\"):\n",
    "                            st.json(result[\"sources\"])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    st.error(f\"❌ Error procesando la consulta: {str(e)}\")\n",
    "                    response = \"Disculpa, ocurrió un error técnico. Por favor intenta nuevamente.\"\n",
    "                    response_time = time.time() - start_time\n",
    "        else:\n",
    "            response = \"El sistema Eureka no está inicializado correctamente. Por favor, revisa los logs.\"\n",
    "    \n",
    "    # Agregar respuesta al historial\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    # Actualizar las métricas en el sidebar\n",
    "    st.rerun()\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"\"\"\n",
    "<div style='text-align: center; color: #666; font-size: 0.8em;'>\n",
    "    <p>🌿 Eureka - Asistente Virtual de la ANLA | Desarrollado con ❤️ para la comunidad</p>\n",
    "    <p>📞 Línea de atención: (601) 254 8888 | 🌐 <a href=\"https://www.anla.gov.co\">www.anla.gov.co</a></p>\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 6: Instrucciones de Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Instrucciones para Ejecutar Eureka Optimizado\n",
    "\n",
    "### 1. Verificar Dependencias\n",
    "Asegúrate de que Ollama esté instalado y funcionando:\n",
    "```bash\n",
    "ollama --version\n",
    "ollama pull llama3.2:latest\n",
    "ollama pull mxbai-embed-large\n",
    "```\n",
    "\n",
    "### 2. Ejecutar la Aplicación\n",
    "En la terminal del directorio del notebook:\n",
    "```bash\n",
    "streamlit run app_eureka_optimized.py\n",
    "```\n",
    "\n",
    "### 3. Características Nuevas\n",
    "- ✅ Manejo mejorado de errores\n",
    "- ✅ Sistema de logging y métricas  \n",
    "- ✅ Búsqueda MMR para mejor relevancia\n",
    "- ✅ Base de conocimiento expandida\n",
    "- ✅ UI mejorada con métricas en tiempo real\n",
    "- ✅ Modo debug para desarrolladores\n",
    "- ✅ Validación automatizada\n",
    "- ✅ Arquitectura modular y escalable\n",
    "\n",
    "### 4. Configuración Avanzada\n",
    "Puedes ajustar parámetros en `EurekaConfig`:\n",
    "- `chunk_size`: Tamaño de fragmentos de texto\n",
    "- `retriever_k`: Número de documentos a recuperar\n",
    "- `temperature`: Creatividad del modelo (0.0 = determinista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principales Optimizaciones Implementadas:\n",
    "\n",
    "### 🔧 **Técnicas**\n",
    "1. **Arquitectura modular**: Clase `EurekaRAG` reutilizable\n",
    "2. **Búsqueda MMR**: Mejor diversidad y relevancia\n",
    "3. **Caché inteligente**: Evita recalcular embeddings\n",
    "4. **Manejo robusto de errores**: Logs detallados\n",
    "5. **Configuración centralizada**: Fácil ajuste de parámetros\n",
    "\n",
    "### 📊 **Monitoreo**\n",
    "1. **Métricas en tiempo real**: Tiempo de respuesta, consultas\n",
    "2. **Logging de interacciones**: Para análisis posterior\n",
    "3. **Modo debug**: Visualización de fuentes consultadas\n",
    "4. **Testing automatizado**: Validación de respuestas\n",
    "\n",
    "### 🎨 **UX/UI**\n",
    "1. **Diseño responsive**: Mejor experiencia móvil\n",
    "2. **Indicadores visuales**: Estados de carga, métricas\n",
    "3. **Sidebar informativo**: Contexto y estadísticas\n",
    "4. **CSS personalizado**: Branding coherente\n",
    "\n",
    "### ⚡ **Performance**\n",
    "1. **Chunks optimizados**: 800 caracteres vs 500 originales\n",
    "2. **Overlap inteligente**: 100 caracteres para mejor contexto\n",
    "3. **Temperatura baja**: Respuestas más consistentes (0.1)\n",
    "4. **Retrieval eficiente**: k=5 documentos por consulta\n",
    "\n",
    "Esta versión optimizada es más robusta, escalable y proporciona mejor experiencia tanto para usuarios finales como desarrolladores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}