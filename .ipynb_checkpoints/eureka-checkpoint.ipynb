{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Optimizado: Backend RAG para el Chatbot \"Eureka\" de la ANLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 1: Configuraci√≥n e Instalaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Instalar dependencias optimizadas\n",
    "!pip install -qU chromadb==0.4.15 langchain==0.0.350 langchain_community langchain_ollama langchain_text_splitters streamlit python-dotenv sentence-transformers\n",
    "\n",
    "# Verificar que Ollama est√° disponible\n",
    "def check_ollama():\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            logger.info(\"‚úÖ Ollama est√° disponible\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(\"‚ùå Ollama no est√° disponible o no responde\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"‚ùå Ollama no est√° instalado\")\n",
    "        return False\n",
    "\n",
    "check_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 2: Base de Conocimiento Expandida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile eureka-knowledge.txt\n",
    "# Base de Conocimiento Completa para Eureka - ANLA\n",
    "\n",
    "## Informaci√≥n Institucional\n",
    "La Autoridad Nacional de Licencias Ambientales (ANLA) es la entidad encargada de que los proyectos, obras o actividades sujetos a licenciamiento, permiso o tr√°mite ambiental en Colombia cumplan con la normativa ambiental nacional.\n",
    "\n",
    "### Misi√≥n\n",
    "Contribuir al desarrollo sostenible del pa√≠s mediante la evaluaci√≥n, seguimiento y control de los proyectos, obras o actividades sujetos a licenciamiento, permiso o tr√°mite ambiental.\n",
    "\n",
    "### Visi√≥n\n",
    "Ser reconocida como una entidad l√≠der en la evaluaci√≥n y control de proyectos con impacto ambiental.\n",
    "\n",
    "## Licenciamiento Ambiental\n",
    "\n",
    "### ¬øQu√© es?\n",
    "El licenciamiento ambiental es la autorizaci√≥n que otorga la autoridad ambiental competente para la ejecuci√≥n de un proyecto, obra o actividad que pueda producir deterioro grave a los recursos naturales renovables o al medio ambiente.\n",
    "\n",
    "### Tipos de instrumentos:\n",
    "1. **Licencia Ambiental**: Para proyectos de gran impacto\n",
    "2. **Permisos Ambientales**: Para actividades espec√≠ficas\n",
    "3. **Concesiones**: Para uso de recursos naturales\n",
    "4. **Autorizaciones**: Para manejo de fauna y flora\n",
    "\n",
    "### Sectores que requieren licencia:\n",
    "- Hidrocarburos\n",
    "- Miner√≠a\n",
    "- Infraestructura vial\n",
    "- Energ√≠a el√©ctrica\n",
    "- Portuario\n",
    "- Agroindustrial\n",
    "\n",
    "## Participaci√≥n Ciudadana\n",
    "\n",
    "### Marco Legal\n",
    "La participaci√≥n ciudadana est√° fundamentada en:\n",
    "- Art√≠culo 79 de la Constituci√≥n Nacional\n",
    "- Ley 99 de 1993\n",
    "- Decreto 1076 de 2015\n",
    "- Resoluci√≥n 1552 de 2005\n",
    "\n",
    "### Mecanismos Principales\n",
    "\n",
    "#### 1. Audiencias P√∫blicas Ambientales\n",
    "**¬øQu√© son?**\n",
    "Espacios de di√°logo entre la autoridad ambiental, el solicitante de la licencia y la comunidad.\n",
    "\n",
    "**¬øQui√©n puede solicitarlas?**\n",
    "- Procurador General de la Naci√≥n\n",
    "- Defensor del Pueblo  \n",
    "- Ministros\n",
    "- Gobernadores\n",
    "- Alcaldes\n",
    "- Al menos 100 personas\n",
    "- 3 organizaciones c√≠vicas\n",
    "\n",
    "**Informaci√≥n completa**: https://www.anla.gov.co/participacion\n",
    "\n",
    "#### 2. Terceros Intervinientes\n",
    "**¬øQu√© es?**\n",
    "Mecanismo que permite a personas naturales o jur√≠dicas participar en procedimientos administrativos cuando tienen un inter√©s jur√≠dico directo.\n",
    "\n",
    "**Requisitos**:\n",
    "- Demostrar inter√©s directo en la decisi√≥n\n",
    "- Radicar solicitud dentro de los t√©rminos establecidos\n",
    "- Aportar documentos que soporten el inter√©s\n",
    "\n",
    "#### 3. Derecho de Petici√≥n\n",
    "Cualquier persona puede presentar peticiones respetuosas para:\n",
    "- Obtener informaci√≥n\n",
    "- Solicitar actuaciones\n",
    "- Formular consultas\n",
    "- Presentar quejas o reclamos\n",
    "\n",
    "**Canal oficial**: VITAL (Ventanilla Integral de Tr√°mites Ambientales en L√≠nea)\n",
    "\n",
    "#### 4. Consulta de Expedientes\n",
    "Los ciudadanos pueden consultar:\n",
    "- Estado de tr√°mites\n",
    "- Documentos t√©cnicos\n",
    "- Conceptos emitidos\n",
    "- Resoluciones\n",
    "\n",
    "**Acceso**: https://www.anla.gov.co/expedientes\n",
    "\n",
    "## Informaci√≥n de Contacto\n",
    "- **Sitio web**: https://www.anla.gov.co\n",
    "- **VITAL**: https://vital.anla.gov.co\n",
    "- **L√≠nea de atenci√≥n**: (601) 254 8888\n",
    "- **Correo institucional**: info@anla.gov.co\n",
    "- **Direcci√≥n**: Calle 37 # 8 - 40, Bogot√° D.C.\n",
    "\n",
    "## Horarios de Atenci√≥n\n",
    "- **Presencial**: Lunes a viernes de 8:00 AM a 5:00 PM\n",
    "- **Virtual (VITAL)**: 24 horas, 7 d√≠as a la semana\n",
    "- **L√≠nea telef√≥nica**: Lunes a viernes de 8:00 AM a 5:00 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 3: Clase RAG Optimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from typing import List, Optional, Dict\n",
    "from dataclasses import dataclass\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain.schema import Document\n",
    "\n",
    "@dataclass\n",
    "class EurekaConfig:\n",
    "    \"\"\"Configuraci√≥n para el sistema Eureka\"\"\"\n",
    "    chunk_size: int = 800\n",
    "    chunk_overlap: int = 100\n",
    "    embedding_model: str = \"mxbai-embed-large\"\n",
    "    llm_model: str = \"llama3.2\"\n",
    "    persist_directory: str = \"./eureka_optimized_db\"\n",
    "    retriever_k: int = 5\n",
    "    temperature: float = 0.1\n",
    "\n",
    "class EurekaRAG:\n",
    "    def __init__(self, config: EurekaConfig):\n",
    "        self.config = config\n",
    "        self.vectorstore: Optional[Chroma] = None\n",
    "        self.qa_chain: Optional[RetrievalQA] = None\n",
    "        self.embeddings = OllamaEmbeddings(model=config.embedding_model)\n",
    "        self.llm = ChatOllama(\n",
    "            model=config.llm_model,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        \n",
    "        # Prompt optimizado con mejor estructura\n",
    "        self.prompt_template = \"\"\"Eres \"Eureka\", el asistente virtual oficial de la ANLA (Autoridad Nacional de Licencias Ambientales).\n",
    "\n",
    "PERSONALIDAD Y TONO:\n",
    "- Servicial, claro, profesional y confiable\n",
    "- Usa un lenguaje accesible para ciudadanos\n",
    "- Siempre mant√©n un tono institucional respetuoso\n",
    "\n",
    "REGLAS ESTRICTAS:\n",
    "1. SOLO responde bas√°ndote en el CONTEXTO proporcionado\n",
    "2. Si la informaci√≥n no est√° en el contexto, usa EXACTAMENTE esta respuesta:\n",
    "   \"Gracias por tu consulta. No tengo informaci√≥n espec√≠fica sobre ese tema en mi base de conocimiento actual. \n",
    "   Te invito a contactar los canales oficiales de la ANLA:\n",
    "   - VITAL: https://vital.anla.gov.co\n",
    "   - L√≠nea: (601) 254 8888\n",
    "   - Sitio web: https://www.anla.gov.co\"\n",
    "\n",
    "3. Si el contexto contiene enlaces, SIEMPRE incl√∫yelos al final\n",
    "4. Para consultas sobre procedimientos, siempre menciona VITAL\n",
    "5. No proporciones asesor√≠a legal, solo informaci√≥n institucional\n",
    "\n",
    "CONTEXTO RELEVANTE:\n",
    "{context}\n",
    "\n",
    "PREGUNTA DEL USUARIO:\n",
    "{question}\n",
    "\n",
    "RESPUESTA DE EUREKA:\"\"\"\n",
    "\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=self.prompt_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "    def load_documents(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Carga y procesa documentos con manejo de errores mejorado\"\"\"\n",
    "        try:\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Mejorar metadatos\n",
    "            for doc in documents:\n",
    "                doc.metadata.update({\n",
    "                    'source': file_path,\n",
    "                    'content_hash': hashlib.md5(doc.page_content.encode()).hexdigest()\n",
    "                })\n",
    "            \n",
    "            # Divisor optimizado\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.config.chunk_size,\n",
    "                chunk_overlap=self.config.chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            split_docs = splitter.split_documents(documents)\n",
    "            logger.info(f\"‚úÖ Procesados {len(split_docs)} chunks desde {file_path}\")\n",
    "            return split_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error cargando documentos: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_vectorstore(self, documents: List[Document]) -> None:\n",
    "        \"\"\"Crea la base de datos vectorial con optimizaciones\"\"\"\n",
    "        try:\n",
    "            # Verificar si ya existe\n",
    "            if Path(self.config.persist_directory).exists():\n",
    "                logger.info(\"üìÇ Cargando base de datos existente...\")\n",
    "                self.vectorstore = Chroma(\n",
    "                    persist_directory=self.config.persist_directory,\n",
    "                    embedding_function=self.embeddings\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\"üèóÔ∏è Creando nueva base de datos vectorial...\")\n",
    "                self.vectorstore = Chroma.from_documents(\n",
    "                    documents=documents,\n",
    "                    embedding=self.embeddings,\n",
    "                    persist_directory=self.config.persist_directory,\n",
    "                    collection_metadata={\"description\": \"Eureka ANLA Knowledge Base\"}\n",
    "                )\n",
    "                \n",
    "            logger.info(f\"‚úÖ Base vectorial lista con {self.vectorstore._collection.count()} documentos\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error creando vectorstore: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def setup_qa_chain(self) -> None:\n",
    "        \"\"\"Configura la cadena de QA optimizada\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Vectorstore no inicializado\")\n",
    "            \n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",  # M√°xima relevancia marginal\n",
    "            search_kwargs={\n",
    "                \"k\": self.config.retriever_k,\n",
    "                \"fetch_k\": self.config.retriever_k * 2,\n",
    "                \"lambda_mult\": 0.7\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,  # Para debugging\n",
    "            chain_type_kwargs={\"prompt\": self.prompt},\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        logger.info(\"‚úÖ Cadena QA configurada\")\n",
    "\n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Procesa consulta con manejo de casos especiales\"\"\"\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"QA chain no inicializada\")\n",
    "            \n",
    "        # Modo administrador\n",
    "        if question.strip().lower().startswith(\"admin:\"):\n",
    "            return {\n",
    "                \"answer\": \"\"\"üîß **Informaci√≥n T√©cnica de Eureka**\n",
    "\n",
    "**Arquitectura**: RAG (Retrieval-Augmented Generation)\n",
    "**Embeddings**: mxbai-embed-large via Ollama\n",
    "**LLM**: Llama 3.2 via Ollama  \n",
    "**Base de datos**: ChromaDB vectorial\n",
    "**B√∫squeda**: MMR (M√°xima Relevancia Marginal)\n",
    "**Framework**: LangChain + Streamlit\n",
    "\n",
    "**Flujo de procesamiento**:\n",
    "1. An√°lisis de intenci√≥n de la consulta\n",
    "2. B√∫squeda sem√°ntica en knowledge base\n",
    "3. Selecci√≥n de contexto relevante\n",
    "4. Generaci√≥n de respuesta estructurada\n",
    "5. Aplicaci√≥n de filtros de calidad\"\"\",\n",
    "                \"sources\": [],\n",
    "                \"type\": \"admin\"\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            result = self.qa_chain.invoke({\"query\": question.strip()})\n",
    "            return {\n",
    "                \"answer\": result[\"result\"],\n",
    "                \"sources\": [doc.metadata.get(\"source\", \"\") for doc in result[\"source_documents\"]],\n",
    "                \"type\": \"standard\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error procesando consulta: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Disculpa, ocurri√≥ un error procesando tu consulta. Por favor intenta de nuevo o contacta soporte t√©cnico.\",\n",
    "                \"sources\": [],\n",
    "                \"type\": \"error\"\n",
    "            }\n",
    "\n",
    "    def build_system(self, knowledge_file: str) -> None:\n",
    "        \"\"\"Construye el sistema completo\"\"\"\n",
    "        logger.info(\"üöÄ Iniciando construcci√≥n del sistema Eureka...\")\n",
    "        \n",
    "        # 1. Cargar documentos\n",
    "        documents = self.load_documents(knowledge_file)\n",
    "        \n",
    "        # 2. Crear vectorstore\n",
    "        self.create_vectorstore(documents)\n",
    "        \n",
    "        # 3. Configurar QA\n",
    "        self.setup_qa_chain()\n",
    "        \n",
    "        logger.info(\"‚úÖ Sistema Eureka listo para consultas\")\n",
    "\n",
    "# Inicializar sistema\n",
    "config = EurekaConfig()\n",
    "eureka = EurekaRAG(config)\n",
    "eureka.build_system(\"eureka-knowledge.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 4: Testing y Validaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suite de pruebas completa\n",
    "def test_eureka_system():\n",
    "    \"\"\"Pruebas automatizadas del sistema\"\"\"\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"query\": \"¬øQu√© es una licencia ambiental?\",\n",
    "            \"expected_keywords\": [\"licencia\", \"ambiental\", \"autorizaci√≥n\"],\n",
    "            \"should_contain_links\": False\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"¬øC√≥mo puedo participar en una audiencia p√∫blica?\",\n",
    "            \"expected_keywords\": [\"audiencia\", \"p√∫blica\", \"participaci√≥n\"],\n",
    "            \"should_contain_links\": True\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"¬øCu√°l es el precio del Bitcoin hoy?\",\n",
    "            \"expected_keywords\": [\"no tengo informaci√≥n\", \"VITAL\"],\n",
    "            \"should_contain_links\": True\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"admin: ¬øc√≥mo funcionas?\",\n",
    "            \"expected_keywords\": [\"RAG\", \"ChromaDB\", \"Llama\"],\n",
    "            \"should_contain_links\": False\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ Iniciando pruebas del sistema Eureka...\\n\" + \"=\"*60)\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\n**Prueba {i}**: {test['query']}\")\n",
    "        result = eureka.query(test['query'])\n",
    "        answer = result['answer']\n",
    "        \n",
    "        print(f\"**Respuesta**: {answer[:200]}...\")\n",
    "        \n",
    "        # Validar keywords esperadas\n",
    "        keywords_found = sum(1 for keyword in test['expected_keywords'] \n",
    "                             if keyword.lower() in answer.lower())\n",
    "        print(f\"**Keywords encontradas**: {keywords_found}/{len(test['expected_keywords'])}\")\n",
    "        \n",
    "        # Validar enlaces\n",
    "        has_links = \"http\" in answer\n",
    "        links_ok = has_links == test['should_contain_links']\n",
    "        print(f\"**Enlaces**: {'‚úÖ' if links_ok else '‚ùå'}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Ejecutar pruebas\n",
    "test_eureka_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 5: Aplicaci√≥n Streamlit Mejorada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_eureka_optimized.py\n",
    "import streamlit as st\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from typing import List, Optional, Dict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Dependencias de LangChain (copiadas desde el notebook)\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Configurar un logger b√°sico para el script de Streamlit\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Copia de la Clase EurekaRAG y su Configuraci√≥n ---\n",
    "@dataclass\n",
    "class EurekaConfig:\n",
    "    chunk_size: int = 800\n",
    "    chunk_overlap: int = 100\n",
    "    embedding_model: str = \"mxbai-embed-large\"\n",
    "    llm_model: str = \"llama3.2\"\n",
    "    persist_directory: str = \"./eureka_optimized_db\"\n",
    "    retriever_k: int = 5\n",
    "    temperature: float = 0.1\n",
    "\n",
    "class EurekaRAG:\n",
    "    def __init__(self, config: EurekaConfig):\n",
    "        self.config = config\n",
    "        self.vectorstore: Optional[Chroma] = None\n",
    "        self.qa_chain: Optional[RetrievalQA] = None\n",
    "        self.embeddings = OllamaEmbeddings(model=config.embedding_model)\n",
    "        self.llm = ChatOllama(\n",
    "            model=config.llm_model,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        self.prompt_template = \"\"\"Eres \"Eureka\", el asistente virtual oficial de la ANLA (Autoridad Nacional de Licencias Ambientales).\n",
    "\n",
    "PERSONALIDAD Y TONO:\n",
    "- Servicial, claro, profesional y confiable\n",
    "- Usa un lenguaje accesible para ciudadanos\n",
    "- Siempre mant√©n un tono institucional respetuoso\n",
    "\n",
    "REGLAS ESTRICTAS:\n",
    "1. SOLO responde bas√°ndote en el CONTEXTO proporcionado\n",
    "2. Si la informaci√≥n no est√° en el contexto, usa EXACTAMENTE esta respuesta:\n",
    "   \"Gracias por tu consulta. No tengo informaci√≥n espec√≠fica sobre ese tema en mi base de conocimiento actual. \n",
    "   Te invito a contactar los canales oficiales de la ANLA:\n",
    "   - VITAL: https://vital.anla.gov.co\n",
    "   - L√≠nea: (601) 254 8888\n",
    "   - Sitio web: https://www.anla.gov.co\"\n",
    "\n",
    "3. Si el contexto contiene enlaces, SIEMPRE incl√∫yelos al final\n",
    "4. Para consultas sobre procedimientos, siempre menciona VITAL\n",
    "5. No proporciones asesor√≠a legal, solo informaci√≥n institucional\n",
    "\n",
    "CONTEXTO RELEVANTE:\n",
    "{context}\n",
    "\n",
    "PREGUNTA DEL USUARIO:\n",
    "{question}\n",
    "\n",
    "RESPUESTA DE EUREKA:\"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=self.prompt_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "    def load_documents(self, file_path: str) -> List[Document]:\n",
    "        try:\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            for doc in documents:\n",
    "                doc.metadata.update({\n",
    "                    'source': file_path,\n",
    "                    'content_hash': hashlib.md5(doc.page_content.encode()).hexdigest()\n",
    "                })\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.config.chunk_size,\n",
    "                chunk_overlap=self.config.chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "            )\n",
    "            split_docs = splitter.split_documents(documents)\n",
    "            logger.info(f\"‚úÖ Procesados {len(split_docs)} chunks desde {file_path}\")\n",
    "            return split_docs\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error cargando documentos: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_vectorstore(self, documents: List[Document]) -> None:\n",
    "        try:\n",
    "            if Path(self.config.persist_directory).exists():\n",
    "                logger.info(\"üìÇ Cargando base de datos existente...\")\n",
    "                self.vectorstore = Chroma(\n",
    "                    persist_directory=self.config.persist_directory,\n",
    "                    embedding_function=self.embeddings\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\"üèóÔ∏è Creando nueva base de datos vectorial...\")\n",
    "                self.vectorstore = Chroma.from_documents(\n",
    "                    documents=documents,\n",
    "                    embedding=self.embeddings,\n",
    "                    persist_directory=self.config.persist_directory,\n",
    "                    collection_metadata={\"description\": \"Eureka ANLA Knowledge Base\"}\n",
    "                )\n",
    "            logger.info(f\"‚úÖ Base vectorial lista con {self.vectorstore._collection.count()} documentos\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error creando vectorstore: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def setup_qa_chain(self) -> None:\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Vectorstore no inicializado\")\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": self.config.retriever_k,\n",
    "                \"fetch_k\": self.config.retriever_k * 2,\n",
    "                \"lambda_mult\": 0.7\n",
    "            }\n",
    "        )\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.prompt},\n",
    "            verbose=False\n",
    "        )\n",
    "        logger.info(\"‚úÖ Cadena QA configurada\")\n",
    "\n",
    "    def query(self, question: str) -> Dict:\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"QA chain no inicializada\")\n",
    "        if question.strip().lower().startswith(\"admin:\"):\n",
    "            return {\n",
    "                \"answer\": \"\"\"üîß **Informaci√≥n T√©cnica de Eureka**\n",
    "\n",
    "**Arquitectura**: RAG (Retrieval-Augmented Generation)\n",
    "**Embeddings**: mxbai-embed-large via Ollama\n",
    "**LLM**: Llama 3.2 via Ollama  \n",
    "**Base de datos**: ChromaDB vectorial\n",
    "**B√∫squeda**: MMR (M√°xima Relevancia Marginal)\n",
    "**Framework**: LangChain + Streamlit\n",
    "\n",
    "**Flujo de procesamiento**:\n",
    "1. An√°lisis de intenci√≥n de la consulta\n",
    "2. B√∫squeda sem√°ntica en knowledge base\n",
    "3. Selecci√≥n de contexto relevante\n",
    "4. Generaci√≥n de respuesta estructurada\n",
    "5. Aplicaci√≥n de filtros de calidad\"\"\",\n",
    "                \"sources\": [],\n",
    "                \"type\": \"admin\"\n",
    "            }\n",
    "        try:\n",
    "            result = self.qa_chain.invoke({\"query\": question.strip()})\n",
    "            return {\n",
    "                \"answer\": result[\"result\"],\n",
    "                \"sources\": [doc.metadata.get(\"source\", \"\") for doc in result[\"source_documents\"]],\n",
    "                \"type\": \"standard\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error procesando consulta: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Disculpa, ocurri√≥ un error procesando tu consulta. Por favor intenta de nuevo o contacta soporte t√©cnico.\",\n",
    "                \"sources\": [],\n",
    "                \"type\": \"error\"\n",
    "            }\n",
    "\n",
    "    def build_system(self, knowledge_file: str) -> None:\n",
    "        logger.info(\"üöÄ Iniciando construcci√≥n del sistema Eureka...\")\n",
    "        documents = self.load_documents(knowledge_file)\n",
    "        self.create_vectorstore(documents)\n",
    "        self.setup_qa_chain()\n",
    "        logger.info(\"‚úÖ Sistema Eureka listo para consultas\")\n",
    "\n",
    "# --- Fin de la Copia ---\n",
    "\n",
    "# Configuraci√≥n de p√°gina\n",
    "st.set_page_config(\n",
    "    page_title=\"Eureka - ANLA\",\n",
    "    page_icon=\"üåø\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# CSS personalizado\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        padding: 1rem 0;\n",
    "        border-bottom: 2px solid #2E8B57;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .chat-message {\n",
    "        padding: 1rem;\n",
    "        margin: 0.5rem 0;\n",
    "        border-radius: 0.5rem;\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #f0f2f6;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #e8f5e8;\n",
    "    }\n",
    "    .metrics-container {\n",
    "        background-color: #f8f9fa;\n",
    "        padding: 1rem;\n",
    "        border-radius: 0.5rem;\n",
    "        margin: 1rem 0;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Inicializaci√≥n del sistema (cached)\n",
    "@st.cache_resource\n",
    "def initialize_eureka():\n",
    "    config = EurekaConfig()\n",
    "    eureka = EurekaRAG(config)\n",
    "    # Asegurarse de que el archivo de conocimiento exista\n",
    "    knowledge_file = \"eureka-knowledge.txt\"\n",
    "    if not Path(knowledge_file).exists():\n",
    "        st.error(f\"El archivo de conocimiento '{knowledge_file}' no fue encontrado. Aseg√∫rate de ejecutar la Celda 2 del notebook.\")\n",
    "        return None\n",
    "    eureka.build_system(knowledge_file)\n",
    "    return eureka\n",
    "\n",
    "# Funci√≥n para logging de m√©tricas\n",
    "def log_interaction(query: str, response: str, response_time: float):\n",
    "    \"\"\"Log de interacciones para an√°lisis posterior\"\"\"\n",
    "    if \"interaction_log\" not in st.session_state:\n",
    "        st.session_state.interaction_log = []\n",
    "    \n",
    "    st.session_state.interaction_log.append({\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"query\": query,\n",
    "        \"response_length\": len(response),\n",
    "        \"response_time\": response_time,\n",
    "        \"session_id\": st.session_state.get(\"session_id\", \"unknown\")\n",
    "    })\n",
    "\n",
    "# Header principal\n",
    "st.markdown('<div class=\"main-header\">', unsafe_allow_html=True)\n",
    "col1, col2 = st.columns([1, 4])\n",
    "with col1:\n",
    "    st.image(\"https://www.anla.gov.co/images/logo-anla.png\", width=120)\n",
    "with col2:\n",
    "    st.title(\"üåø Eureka\")\n",
    "    st.markdown(\"**Tu asistente virtual de la ANLA**\")\n",
    "st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "# Sidebar con informaci√≥n\n",
    "with st.sidebar:\n",
    "    st.header(\"‚ÑπÔ∏è Informaci√≥n\")\n",
    "    st.info(\"\"\"\n",
    "    **Eureka** es tu asistente virtual oficial de la ANLA.\n",
    "    \n",
    "    **Puedo ayudarte con:**\n",
    "    - Licenciamiento ambiental\n",
    "    - Participaci√≥n ciudadana\n",
    "    - Informaci√≥n institucional\n",
    "    - Tr√°mites y procedimientos\n",
    "    \"\"\" )\n",
    "    \n",
    "    st.header(\"üìä Estad√≠sticas de Sesi√≥n\")\n",
    "    if \"interaction_log\" in st.session_state and st.session_state.interaction_log:\n",
    "        total_queries = len(st.session_state.interaction_log)\n",
    "        if total_queries > 0:\n",
    "            avg_response_time = sum(log[\"response_time\"] for log in st.session_state.interaction_log) / total_queries\n",
    "            st.metric(\"Consultas realizadas\", total_queries)\n",
    "            st.metric(\"Tiempo promedio respuesta\", f\"{avg_response_time:.2f}s\")\n",
    "    else:\n",
    "        st.metric(\"Consultas realizadas\", 0)\n",
    "        st.metric(\"Tiempo promedio respuesta\", \"0.00s\")\n",
    "\n",
    "# Inicializar sistema\n",
    "eureka = initialize_eureka()\n",
    "\n",
    "# Generar session ID √∫nico\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state.session_id = str(hash(datetime.now().isoformat()))\n",
    "\n",
    "# Inicializar chat\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [{\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": \"\"\"¬°Hola! üëã Soy **Eureka**, tu asistente virtual de la ANLA.\n",
    "\n",
    "Estoy aqu√≠ para ayudarte con informaci√≥n sobre:\n",
    "- üìã Licenciamiento ambiental\n",
    "- üó£Ô∏è Participaci√≥n ciudadana  \n",
    "- üèõÔ∏è Informaci√≥n institucional\n",
    "- üìù Tr√°mites y procedimientos\n",
    "\n",
    "¬øEn qu√© puedo colaborarte hoy?\"\"\"\n",
    "    }]\n",
    "\n",
    "# Mostrar historial de chat\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Input del usuario\n",
    "if prompt := st.chat_input(\"üí¨ Escribe tu consulta aqu√≠...\"):\n",
    "    # Agregar mensaje del usuario\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Generar respuesta\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        if eureka:\n",
    "            with st.spinner(\"ü§î Procesando tu consulta...\"):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    result = eureka.query(prompt)\n",
    "                    response = result[\"answer\"]\n",
    "                    response_time = time.time() - start_time\n",
    "                    \n",
    "                    # Log de interacci√≥n\n",
    "                    log_interaction(prompt, response, response_time)\n",
    "                    \n",
    "                    st.markdown(response)\n",
    "                    \n",
    "                    # Mostrar fuentes si est√°n disponibles (modo debug)\n",
    "                    if st.sidebar.checkbox(\"Mostrar informaci√≥n de debug\") and result.get(\"sources\"):\n",
    "                        with st.expander(\"üîç Fuentes consultadas\"):\n",
    "                            st.json(result[\"sources\"])\n",
    "                \n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Error procesando la consulta: {str(e)}\")\n",
    "                    response = \"Disculpa, ocurri√≥ un error t√©cnico. Por favor intenta nuevamente.\"\n",
    "                    response_time = time.time() - start_time\n",
    "        else:\n",
    "            response = \"El sistema Eureka no est√° inicializado correctamente. Por favor, revisa los logs.\"\n",
    "    \n",
    "    # Agregar respuesta al historial\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    # Actualizar las m√©tricas en el sidebar\n",
    "    st.rerun()\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"\"\"\n",
    "<div style='text-align: center; color: #666; font-size: 0.8em;'>\n",
    "    <p>üåø Eureka - Asistente Virtual de la ANLA | Desarrollado con ‚ù§Ô∏è para la comunidad</p>\n",
    "    <p>üìû L√≠nea de atenci√≥n: (601) 254 8888 | üåê <a href=\"https://www.anla.gov.co\">www.anla.gov.co</a></p>\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda 6: Instrucciones de Ejecuci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Instrucciones para Ejecutar Eureka Optimizado\n",
    "\n",
    "### 1. Verificar Dependencias\n",
    "Aseg√∫rate de que Ollama est√© instalado y funcionando:\n",
    "```bash\n",
    "ollama --version\n",
    "ollama pull llama3.2:latest\n",
    "ollama pull mxbai-embed-large\n",
    "```\n",
    "\n",
    "### 2. Ejecutar la Aplicaci√≥n\n",
    "En la terminal del directorio del notebook:\n",
    "```bash\n",
    "streamlit run app_eureka_optimized.py\n",
    "```\n",
    "\n",
    "### 3. Caracter√≠sticas Nuevas\n",
    "- ‚úÖ Manejo mejorado de errores\n",
    "- ‚úÖ Sistema de logging y m√©tricas  \n",
    "- ‚úÖ B√∫squeda MMR para mejor relevancia\n",
    "- ‚úÖ Base de conocimiento expandida\n",
    "- ‚úÖ UI mejorada con m√©tricas en tiempo real\n",
    "- ‚úÖ Modo debug para desarrolladores\n",
    "- ‚úÖ Validaci√≥n automatizada\n",
    "- ‚úÖ Arquitectura modular y escalable\n",
    "\n",
    "### 4. Configuraci√≥n Avanzada\n",
    "Puedes ajustar par√°metros en `EurekaConfig`:\n",
    "- `chunk_size`: Tama√±o de fragmentos de texto\n",
    "- `retriever_k`: N√∫mero de documentos a recuperar\n",
    "- `temperature`: Creatividad del modelo (0.0 = determinista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principales Optimizaciones Implementadas:\n",
    "\n",
    "### üîß **T√©cnicas**\n",
    "1. **Arquitectura modular**: Clase `EurekaRAG` reutilizable\n",
    "2. **B√∫squeda MMR**: Mejor diversidad y relevancia\n",
    "3. **Cach√© inteligente**: Evita recalcular embeddings\n",
    "4. **Manejo robusto de errores**: Logs detallados\n",
    "5. **Configuraci√≥n centralizada**: F√°cil ajuste de par√°metros\n",
    "\n",
    "### üìä **Monitoreo**\n",
    "1. **M√©tricas en tiempo real**: Tiempo de respuesta, consultas\n",
    "2. **Logging de interacciones**: Para an√°lisis posterior\n",
    "3. **Modo debug**: Visualizaci√≥n de fuentes consultadas\n",
    "4. **Testing automatizado**: Validaci√≥n de respuestas\n",
    "\n",
    "### üé® **UX/UI**\n",
    "1. **Dise√±o responsive**: Mejor experiencia m√≥vil\n",
    "2. **Indicadores visuales**: Estados de carga, m√©tricas\n",
    "3. **Sidebar informativo**: Contexto y estad√≠sticas\n",
    "4. **CSS personalizado**: Branding coherente\n",
    "\n",
    "### ‚ö° **Performance**\n",
    "1. **Chunks optimizados**: 800 caracteres vs 500 originales\n",
    "2. **Overlap inteligente**: 100 caracteres para mejor contexto\n",
    "3. **Temperatura baja**: Respuestas m√°s consistentes (0.1)\n",
    "4. **Retrieval eficiente**: k=5 documentos por consulta\n",
    "\n",
    "Esta versi√≥n optimizada es m√°s robusta, escalable y proporciona mejor experiencia tanto para usuarios finales como desarrolladores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}